---
title: "Part 3"
author: "Sara Altman and Hershel Mehta"
date: "12/5/2017"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message=FALSE, warning=FALSE)
```

```{r message=FALSE, warning=FALSE}
library(tidyverse)  
library(modelr)
```


```{r, message=FALSE}
data <- read_csv("relapse_subjects.csv", na = c("", "NaN")) %>% 
  filter(relIn6Mos %in% c(0, 1))
```

#Prediction on the test set

We didn't have enough data for a test set, so we used all our data in Part 2. We used leave-one-out cross-validation to get an estimate of the test error. The results of this process are reported here. 

##Classification

```{r}

test_err <- function(test, mod, threshold) {
  test <- as_tibble(test)
  
  errors <- 
    test %>% 
    mutate(
      pred = predict(mod, newdata = test, type = "response"),
      pred_group = ifelse(pred > threshold, 1, 0),
      pred_error = ifelse(relIn6Mos != pred_group, 1, 0)
    )
  
  return(mean(errors$pred_error))
}

test_err_fn <- function(test, mod, threshold) {
  test <- as_tibble(test)
  
  errors <- 
    test %>% 
    mutate(
      pred = predict(mod, newdata = test, type = "response"),
      pred_group = ifelse(pred > threshold, 1, 0),
      false_neg = ifelse(relIn6Mos > pred_group, 1, 0)
    )
  
  return(mean(errors$false_neg))
}

test_err_fp <- function(test, mod, threshold) {
  test <- as_tibble(test)
  
  errors <- 
    test %>% 
    mutate(
      pred = predict(mod, newdata = test, type = "response"),
      pred_group = ifelse(pred > threshold, 1, 0),
      false_pos = ifelse(relIn6Mos < pred_group, 1, 0)
    )
  
  return(mean(errors$false_pos))
}

set.seed(1)

by_subj_cv <-
  data %>% 
  crossv_kfold(37)

#for threshold = .5
by_subj_cv %>% 
  mutate(train = map(train, as_tibble),
         model = map(train, ~glm(relIn6Mos ~ nacc_neutral_beta,
                                  family = "binomial", 
                                  data = .)),
         error = map2_dbl(test, model, test_err, threshold = .4),
         false_neg_rate = map2_dbl(test, model, test_err_fn, threshold = .4),
         false_pos_rate = map2_dbl(test, model, test_err_fp, threshold = .4)) %>% 
  summarise(`CV error rate` = mean(error),
            `CV false negative rate` = mean(false_neg_rate),
            `CV false positive rate` = mean(false_pos_rate)) %>% 
  knitr::kable(digits = 2)
```

Our estimated test error is .24.  

## Regression

```{r}
data %>% 
  filter(relIn6Mos == 1) %>% 
  crossv_kfold(k = 15) %>% 
  mutate(
    mod = map(train, ~lm(obstime ~ naccR_drugs_beta, data = .)),
    rmse = map2_dbl(mod, test, rmse)
  ) %>% 
  summarise(
    RMSE_CV = mean(rmse)
  ) %>% 
  knitr::kable(digits = 2)
```

Our estimated test RMSE is 34.8.

#Inference

## (a)
### Logistic regression model fitted on all data
```{r}
fit_all_data <- glm(relIn6Mos ~ nacc_neutral_beta,
                    family = "binomial",
                    data = data)

broom::tidy(fit_all_data) %>% 
  knitr::kable()
```

The coefficient on nacc_neutral_beta is significant at the $\alpha = .01$ level. The p-value is .002. This means that, if the null hypothesis is true, the probability of observing a Wald statistic as extreme as 3.046 is .002. Here, the null hypothesis is that the coefficient on `nacc_neutral_beta` is 0. 

We only have one covariate. If the coefficient on this covariate really is 0, then the chance it will be significant at the $\alpha = .05$ level is 5\% (and is 1\% at the $\alpha = .01$ level). Therefore, the chance is low that this one covariate would be significant if the null were true. In contrast, if we had 100 covariates, we would expect 5 of them to be significant at the $\alpha = .05$ level and so we might not believe our results if it turned out that 5 of our coefficient were significant. 

Therefore, we think we can believe our results that the coefficient on `nacc_neutral_beta` is significant. Our p-value may be favorably biased, however, for reasons discussed in part (e).

## (b)
```{r}
set.seed(1)

by_subj_cv <-
  data %>% 
  crossv_kfold(10)

by_subj_cv %>% 
  mutate(train = map(train, as_tibble),
         test = map(test, as_tibble),
         model_tr = map(train, ~glm(relIn6Mos ~ nacc_neutral_beta,
                                  family = "binomial", 
                                  data = .)),
         summary_tr = map(model_tr, summary),
         p_value_tr = map_dbl(summary_tr, ~.$coefficients[8]),
         sig_05 = ifelse(p_value_tr < .05, 1, 0),
         sig_01 = ifelse(p_value_tr < .01, 1, 0),
         sig_001 = ifelse(p_value_tr < .001, 1, 0)) %>% 
  summarise(`Proportion significant (alpha = .05)` = mean(sig_05),
            `Proportion significant (alpha = .01)`  = mean(sig_01),
            `Proportion significant (alpha = .001)`  = mean(sig_001)) %>% 
  knitr::kable()
```

We didn't have enough data for a test set and used cross-validation in the prediction part of the project. So for this part, we again used cross-validation. We created 10 folds and fit our model on the training data associated with each fold. The table above gives the proportion of folds where the coefficient on `nacc_neutral_beta` was significant, for three $\alpha$ levels.

The coefficient on `nacc_neutral_beta` is significant in 100\% of the folds at the $\alpha = .05$ level and at and is significant 90\% of the time at the $\alpha = .01$ level.

## (c)

Our original 95% confidence interval for our `nacc_neutral_beta` coefficient is [11.29, 45.05] as we can see from the table below.

```{r}
confint(fit_all_data) %>% 
  knitr::kable()
```


```{r warning = FALSE, message = FALSE}
mod_glm_bootstrap <- 
  data %>% 
  modelr::bootstrap(10000) %>% 
  mutate(
    mod_glm = map(strap, ~glm(relIn6Mos ~ nacc_neutral_beta, 
                              family = "binomial", 
                              data = .,
                              control = list(maxit = 50)
                              )),
    coef = map_dbl(mod_glm, ~coef(.)[["nacc_neutral_beta"]])
  ) %>% 
  filter(coef <= 100)
```


```{r}
bootstrap_glm_sd <- sd(mod_glm_bootstrap$coef)
```

The following plot shows the bootstrap distribution of our coefficient, with our bootstrapped confidence interval indicated.

```{r}
mod_glm_bootstrap %>% 
  ggplot(aes(x = coef)) +
  geom_histogram() +
  geom_vline(xintercept = 25.5718, color = "blue") +
  geom_vline(xintercept = 25.5718 - 1.96 * bootstrap_glm_sd, color = "red") +
  geom_vline(xintercept = 25.5718 + 1.96 * bootstrap_glm_sd, color = "red") +
  labs(x = "Coefficient",
       y = "Count")
```

When calculating our bootstrapped 95% confidence interval, we run into an error in many of our bootstrapped samples where "fitted probabilities numerically 0 or 1 occurred". We believe this occurs because we have perfect linear separation in some of our bootstrapped samples, which may occur since our n is quite small. This is interesting because it means that in some of our samples, our coefficient perfectly separates all cases of relapsers from non-relapsers, but this may simply occur as a product of bootstrapping on a small sample of data. Also as a result of that, our model converges to very extreme coefficient values, making it difficult to interpret confidence intervals.

To deal with this problem, we attempted to filter out some of the most extreme values (coefficient values >= 100), and made the plot of the 95% confidence interval (i.e., [`r 25.5718 - 1.96 * bootstrap_glm_sd`, `r 25.5718 + 1.96 * bootstrap_glm_sd`]) above. The plot shows a larger 95% confidence interval than our standard glm output, which you'd expect because of the extreme values created by the perfect linear separation issue mentioned above. 

## (d)

```{r}
data_betas <- 
  data %>% 
  select(
    relIn6Mos,
    contains("beta"),
    -starts_with("naccR"),
    -starts_with("naccL")
  )
```

```{r}
mod_glm_betas <- glm(relIn6Mos ~ ., family = "binomial", data = data_betas)

broom::tidy(mod_glm_betas) %>% 
  knitr::kable()
```


We again see the perfect linear separation issue mentioned above when working with all the different beta values. This demonstrates why we took care to build parsimonious models in part 2 and 3, and that when working with small data, you must take a lot of care when adding features. 


To see whether our significant coefficients changed on a larger model, we also created a model that includes the "neutral betas" for all brain regions in our data (the neutal betas are measure of activity when the subject is presented with neutral cues):

```{r}
data_neutral_betas <- 
  data %>% 
  select(
    relIn6Mos,
    contains("neutral_beta"),
    -starts_with("naccR"),
    -starts_with("naccL")
  )
```

```{r}
mod_glm_neutral_betas <- glm(relIn6Mos ~ ., family = "binomial", data = data_neutral_betas)

broom::tidy(mod_glm_neutral_betas) %>% 
  knitr::kable()

cor_nacc_ains <- cor(data_neutral_betas$nacc_neutral_beta, data_neutral_betas$ains_neutral_beta)
```

We can see here that `nacc_neutral_beta` is still significant but its p-value is larger (and its coefficient is smaller) than the original single variable glm above. This makes sense when considering the variables we added are activations during the same cues in different brain regions (e.g., `ains_neutral_beta` are the betas extracted from the anterior insula, a brain region thought to be involved in emotion) and are likely to be correlated. Based on prior knowledge of how these regions are connected, you would expect, for instance, the NAcc and Anterior Insula to be correlated. Indeed, the correlation is `r round(cor_nacc_ains, 1)`.

## (e)

Post selection inference may be a problem here. We selected our model after looking at our data. We chose the model that looked like it fit the data best and gave us a low estimated test error. However, when we test for significance, we do not take this selection process into account. This is likely to favorably bias our p-values. One way we could have dealt with this is by creating $b$ bootstrapped samples, using each to sample to create a model, and then looking at how often our coefficient ended up being significant. This would take our model selection process into account. Another way to dealing with post-selection inference would be to validate on new data. Since we didn't have a test set, we couldn't do this. 

Since we only have one covariate, multiple hypothesis testing is generally not a problem and no corrections are needed. If our coefficient is actually zero (i.e., the null is true), there is only a 5% chance it will be significant at the $\alpha = .05$ level. However, when we used 10-fold cross-validation to get an estimate of how our model performs on test data, we did conduct multiple (10) hypotheses. We should therefore expect 5% of our rejections to be false positives. If we applied a Bonferroni correction here, our probability of declaring even one false positive would be no more than 5%. We didn't think it was particurly relevant, however, to apply a correction in this case, since our reasons for doing cross-validation in part (b) was just to do something equivalent to fitting our model on a test set.

In part (d), we showed that `nacc_neutral_beta` is correlated `ains_neutral_beta` (in other words, NAcc activity is correlated with Anterior Insula activity when a person is presented with neutral cues). However, our model only has one coefficient and so this correlation should not affect our p-values. 


## (f)

It doesn't make sense to interpret the relationship between NAcc activity when looking at neutral cues and relapse in 6 months as causal. Addiction is the underlying mechanism increasing chance of relapse, and there are many complex factors related to addiction that may be confouding our ability to infer causality. For instance, various emotional, personality, and socio-economic factors play a role in addiction, so the variations in these factors may be creating confounds for relapse. We also know (as discussed above) that NAcc activity is correlated with activity in other brain regions. Therefore, we cannot interpret the relationship we discovered causally.

We were not expecting to find a causal relationship, and causal relationship would not make much sense in this context. Our goal was to find a reliable neural signal of risk for relapse within 6 months.

#Discussion

These models could be used for both predication and inference. The models could be used by clinicians to predict if a stimulant-dependent veteran will relapse after rehabilitation. The models could also be used to make inferences about the neuroscience of addiction.

Our models should hold up well over time. There's no obvious reason to suspect that time will affect the relationship between nucleus accumbens activity and relapsing. However, updating our data and then refitting the regression model after more time has passed would be useful. This is because our current analysis removes those who hadn't relapsed at the time the data set was completed. However, this doesn't mean that they never relapsed. It would be useful to continually follow-up with them (if possible), update our data, and then refit our regression model. Note that this isn't relevant to our current classification model, since that predicts relapse in 6 months. 

Users of our models (e.g., clinicians helping stimulant addicts) should be aware that are data set was small and therefore our models are vulnerable to overfitting. Although we used cross-validation to get an estimate of our test error, this is likely still an underestimate of the true test error. Therefore, users of the model should be cautious about making important decisions (e.g., whether or not to give a patient extra treatment or to release them) using our model, and should definitely not rely solely on our models' predictions. 

If we could recollect our data, we would try to increase the number of patients in the study. We would also increase the number of brain regions from which activity was recorded. Our current data includes covariates collected during follow-ups (e.g., obstime, relapse). We would add a survey to assess general well-being to the covariates collected during the follow-up.

If we were to analyse the same data set again, we would try a) using all by-trial data to create a hierarchical model (our current models just use our by-subject data set) and b) use survival analysis to model time to relapse. 